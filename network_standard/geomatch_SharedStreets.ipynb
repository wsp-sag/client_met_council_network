{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geojson import FeatureCollection, Feature\n",
    "import http.client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "anoka_input_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\GIS\\CountyLanes_3-27-19\\Anoka\\AnokaCo_Segments_032519.shp\"\n",
    "hennepin_input_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\GIS\\CountyLanes_3-27-19\\Hennepin\\Roadway_Width_032619.shp\"\n",
    "carver_input_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\GIS\\CountyLanes_3-27-19\\Carver County\\Carver_export.shp\"\n",
    "dakota_input_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\GIS\\CountyLanes_3-27-19\\Dakota\\Dakota_export.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp_to_json(input_gdf):\n",
    "    input_gdf_latlon = input_gdf.copy()\n",
    "    input_gdf_latlon = input_gdf_latlon[input_gdf_latlon.geometry.notnull()]\n",
    "    \n",
    "    input_gdf_latlon['geometry'] = input_gdf_latlon['geometry'].apply(lambda g: LineString([xy[0:2] for xy in list(g.coords)]))\n",
    "    \n",
    "    input_gdf_latlon = input_gdf_latlon.to_crs({'init': 'epsg:4326'})\n",
    "    \n",
    "    obj_list = [x for x in input_gdf_latlon.columns.values if (input_gdf_latlon[x].dtype == object)&(x!='geometry')]\n",
    "    for x in obj_list:\n",
    "        input_gdf_latlon[x] = input_gdf_latlon[x].str.replace('\\'', '')\n",
    "        input_gdf_latlon[x] = input_gdf_latlon[x].str.replace('\\\"', '')\n",
    "    \n",
    "    input_gdf_latlon.fillna(0, inplace = True)\n",
    "        \n",
    "    input_json = json.loads(input_gdf_latlon.to_json())\n",
    "    \n",
    "    return input_json\n",
    "\n",
    "\n",
    "\n",
    "def match_geom(input_json, laneKey, group_size):\n",
    "    input_data = input_json['features']\n",
    "    subset = (np.arange(0, len(input_data), group_size)).tolist() + [len(input_data)]\n",
    "    \n",
    "    unmatched_df = pd.DataFrame()\n",
    "    matched_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(subset)-1):\n",
    "        input_subset = input_data[subset[i] : subset[i+1]]\n",
    "        \n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection(\"api.sharedstreets.io\")    \n",
    "            fc = {\"type\":\"FeatureCollection\", \"features\":input_subset}\n",
    "            payload = str(fc).replace('\\'', '\\\"')\n",
    "    \n",
    "            headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "            conn.request(\"POST\", \n",
    "                     \"/v0.1.0/match/geoms?authKey=bdd23fa1-7ac5-4158-b354-22ec946bb575\", \n",
    "                     payload, \n",
    "                     headers)\n",
    "\n",
    "            res = conn.getresponse()\n",
    "            return_data = res.read()\n",
    "    \n",
    "            return_json = json.loads(return_data.decode('utf-8'))\n",
    "    \n",
    "            if return_json['unmatched'] is not None:\n",
    "                unmatched_df = pd.concat([unmatched_df,\n",
    "                                     pd.DataFrame(return_json['unmatched']['features'])],\n",
    "                                     sort = False,\n",
    "                                     ignore_index = True)\n",
    "    \n",
    "            # matched database\n",
    "            features = return_json['matched']['features']\n",
    "            rows = []\n",
    "            for feature in features:\n",
    "                row = {'referenceId' : feature['properties']['referenceId'],\n",
    "                  'fromIntersectionId' : feature['properties']['fromIntersectionId'],\n",
    "                  'toIntersectionId' : feature['properties']['toIntersectionId'],\n",
    "                  'roadClass' : feature['properties']['roadClass'],\n",
    "                  'direction' : feature['properties']['direction'],\n",
    "                  'referenceLength' : feature['properties']['referenceLength'],\n",
    "                  'side' : feature['properties']['side'],\n",
    "                  'score' : feature['properties'].get('score', None),\n",
    "                  'originalFeatureId' : feature['properties']['originalFeature']['properties']['LINK_ID'],\n",
    "                  'originalFeatureLanes' : feature['properties']['originalFeature']['properties'][laneKey]}\n",
    "            \n",
    "                rows.append(row)\n",
    "    \n",
    "            matched_df = pd.concat([matched_df, \n",
    "                               pd.DataFrame(rows)],\n",
    "                               sort = False,\n",
    "                               ignore_index = True)\n",
    "        \n",
    "        except:\n",
    "            print((subset[i], subset[i+1]))\n",
    "            continue\n",
    "    \n",
    "    unmatched_df['originalFeatureId'] = unmatched_df.properties.apply(lambda x: x['LINK_ID'])\n",
    "    \n",
    "    return matched_df, unmatched_df\n",
    "\n",
    "\n",
    "\n",
    "def main(input_path, matched_dir, unmatched_dir, laneKey, group_size = 200):\n",
    "    print('start: ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    input_gdf = gpd.read_file(input_path)[[laneKey, 'geometry']]\n",
    "    input_gdf['LINK_ID'] = range(1, len(input_gdf) + 1)\n",
    "    input_json = shp_to_json(input_gdf)\n",
    "    matched_df, unmatched_df = match_geom(input_json, laneKey, group_size)\n",
    "    \n",
    "    print('end: ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '\\n')\n",
    "    \n",
    "    matched_df.to_csv(matched_dir, index = False)\n",
    "    unmatched_df.to_csv(unmatched_dir, index = False)\n",
    "    \n",
    "    return matched_df, unmatched_df\n",
    "\n",
    "\n",
    "def retry_match(input_path, previous_matched, previous_unmatched, laneKey, group_size):\n",
    "    previous_matched_df = pd.read_csv(previous_matched)\n",
    "    previous_unmatched_df = pd.read_csv(previous_unmatched)\n",
    "    input_gdf = gpd.read_file(input_path)[[laneKey, 'geometry']]\n",
    "    \n",
    "    input_gdf['LINK_ID'] = range(1, len(input_gdf) + 1)\n",
    "    input_gdf = input_gdf[-input_gdf['LINK_ID'].isin(previous_matched_df.originalFeatureId.unique().tolist())]\n",
    "    input_gdf = input_gdf[-input_gdf['LINK_ID'].isin(previous_unmatched_df.originalFeatureId.unique().tolist())]\n",
    "\n",
    "    input_json = shp_to_json(input_gdf)\n",
    "    \n",
    "    print('start: ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    matched_df, unmatched_df = match_geom(input_json, laneKey, group_size)\n",
    "    \n",
    "    print('end: ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    matched_df = pd.concat([previous_matched_df, matched_df], sort = False, ignore_index = True)\n",
    "    unmatched_df = pd.concat([previous_unmatched_df, unmatched_df], sort = False, ignore_index = True)\n",
    "\n",
    "    matched_df.to_csv(previous_matched, index = False)\n",
    "    unmatched_df.to_csv(previous_unmatched, index = False)\n",
    "    \n",
    "    return matched_df, unmatched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2019-04-13 15:09:30\n",
      "end: 2019-04-13 15:09:51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\matched\\matched_anoka.csv\"\n",
    "unmatched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\unmatched\\unmatched_anoka.csv\"\n",
    "anoka_matched, anoka_unmatched = main(anoka_input_dir, matched_dir, unmatched_dir, 'LANES', 200)\n",
    "#input_gdf = gpd.read_file(anoka_input_dir)\n",
    "#input_json = shp_to_json(input_gdf)\n",
    "#matched_df, unmatched_df = match_geom(input_json, 'LANES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2019-04-13 15:10:01\n",
      "end: 2019-04-13 15:11:09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\matched\\matched_hennepin.csv\"\n",
    "unmatched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\unmatched\\unmatched_hennepin.csv\"\n",
    "hennepin_matched, hennepin_unmatched = main(hennepin_input_dir, matched_dir, unmatched_dir, 'Lanes', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2019-04-13 23:12:05\n",
      "(18, 19)\n",
      "(79, 80)\n",
      "end: 2019-04-13 23:14:12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\matched\\matched_carver.csv\"\n",
    "unmatched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\unmatched\\unmatched_carver.csv\"\n",
    "carver_matched, carver_unmatched = main(carver_input_dir, matched_dir, unmatched_dir, 'Lanes', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2019-04-13 15:11:30\n",
      "end: 2019-04-13 15:11:47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\matched\\matched_dakota.csv\"\n",
    "unmatched_dir = r\"Z:\\Data\\Users\\Sijia\\Met_Council\\osm_roadway_network_builder\\lane_conflation\\unmatched\\unmatched_dakota.csv\"\n",
    "dakota_matched, dakota_unmatched = main(dakota_input_dir, matched_dir, unmatched_dir, 'No_of_Traf', 200)\n",
    "#input_gdf = gpd.read_file(dakota_input_dir)\n",
    "#input_gdf['geometry'] = input_gdf['geometry'].apply(lambda g: LineString([xy[0:2] for xy in list(g.coords)]))\n",
    "#input_gdf = input_gdf.to_crs({'init': 'epsg:4326'})\n",
    "#list(input_gdf.geometry[100].coords)\n",
    "#input_gdf.crs\n",
    "#input_gdf = input_gdf[input_gdf.geometry.notnull()]\n",
    "#input_gdf = input_gdf.to_crs({'init': 'epsg:4326'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (metC)",
   "language": "python",
   "name": "metc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
